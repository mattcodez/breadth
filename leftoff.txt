10-4: knex can't acquire a connection again
10-3: Just do a test site for now, need to parse robots.txt before
    beginning to follow links.
9-27: Schema should be done, start on capture code.
    How to store history of page captures?
    Should we just have multiple "pages" or another child table? URL would
    get repeated. What about when page goes away? Store last status here?
    What about re-directs, which URL do you store then?
    I think fully normalized way is to just store url in pages with any
    redirects and then have a child table for history.
9-26: Can't get knex to connect, env variable not getting set. How does
source command work in bash?
 Thinking cheerio and request for scraping
9-24: Start scraping
9-23:
Function works, so, schema for holding responses?
See if new function works.
New table for domains, use stored procedure to populate
  from domain_staging
9-22: Trim first column of file first
9-21: Peer authentication failed for user "importer"
9-17: Just take the first column, the domain, don't worry about other far out stuff right now.
  Schema should be ready, start on import script.
9-15-16: What columns to I need for zone file import into database?
